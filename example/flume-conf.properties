# canal -> source -> memoryChannel -> kafkaSink

agent.sources = canalSource
agent.channels = memoryChannel
agent.sinks = kafkaSink

agent.sources.canalSource.type = com.citic.source.canal.CanalSource

# get local ip for data monitor
agent.sources.canalSource.ipInterface = eth0

# zookeeper servers
agent.sources.canalSource.zkServers = 192.168.2.25:2181

# canal destination
agent.sources.canalSource.destination = example

# agent.sources.canalSource.username = user
# agent.sources.canalSource.password = password

# binlog batch size, default is 1024
agent.sources.canalSource.batchSize = 1024

agent.sources.canalSource.tableToTopicMap = test.test:test123:schema1;test.test1:test234:schema2
#db.table_name:field_name,field_name;schema.table_name:field_name,field_name
agent.sources.canalSource.tableFieldsFilter = id|id1,name|name1;uid|uid2,name|name2

agent.sources.canalSource.useAvro = true


agent.sources.canalSource.channels = memoryChannel

agent.sinks.kafkaSink.channel = memoryChannel
agent.sinks.kafkaSink.type = org.apache.flume.sink.kafka.KafkaSink

agent.sinks.kafkaSink.allowTopicOverride = true
agent.sinks.kafkaSink.topicHeader = topic
agent.sinks.kafkaSink.flumeBatchSize = 200
agent.sinks.kafkaSink.useAvroEventFormat = true

agent.sinks.kafkaSink.kafka.topic = canal_test
agent.sinks.kafkaSink.kafka.bootstrap.servers = 192.168.2.25:9092
agent.sinks.kafkaSink.kafka.registryUrl = http://localhost:8081
agent.sinks.kafkaSink.kafka.sendErrorFile = log/send-error.log
agent.sinks.kafkaSink.kafka.producer.acks = 1
agent.sinks.kafkaSink.kafka.producer.linger.ms = 1
agent.sinks.kafkaSink.kafka.producer.retries = 3
agent.sinks.kafkaSink.kafka.producer.compression.type = snappy


agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity = 100
